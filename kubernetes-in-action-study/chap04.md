# 4. 레블리케이션과 그 밖의 컨트롤러: 팟 배포 관리

```
- 팟을 안정적으로 유지
- 동일한 팟의 여러 인스턴스 실행
- 노드 실패 시 자동으로 팟 다시 스케줄링
- 수평으로 팟 스케일링 
- 각 클러스터 노드에서 시스템 수준 팟 실행
- 배치
- 작업을 주기적으로 또는 한 번씩 실행하도록 스케줄링
```

## 실제 환경에서는 배포한 애클리케이션이 자동으로 로드하여 실행되고 수동으로 개입하지 않아도 정상적으로 안정적 상태를 유지하길 원할 것이다. 래플리케이션 컨트롤러 또는 디플로이먼트 같은 유형의 리소스를 생성해 실제 포드를 관리하도로 한다.  

## 1. 팟을 안정적으로 유지
- 팟이 노드에 스케줄되는 대로, 해당 노드의 Kubeletdms 컨테이너를 실행하고 팟이 존재하는 한 계속 실행된다.  
`컨테이너 크래시 발생하면 Kubelet 이 컨테이너를 다시 시작 해준다.`  
`어플리케이션 버그 발생시 쿠버네티스가 자동으로 다시 시작후 자동 복구`  
`어플리케이션이 무한루프나 교착상태에 빠져 응답을 멈추는 상태라면 외부에서 체크 하도록 해야함`  

### 1.1 라이브니스 프로브 소개
- 쿠버네티스는 라이브니스 프로브를 통해 컨테이너 헬스 체크
- 팟 사양에서 각 컨테이너에 라이브니스 프로브를 지정할 수 있다.
- 쿠버네티스는 세가지중 하나를 사용해 컨테이너를 검색한다.  
A. HTTP GET 프로브는 지정한 IP 주소로 HTTP GET을  요청한다.  Status 2xx 또는 3xx 을 응답하지 않으면 컨테이너  재시작  
B. TCP 소켓 프로브가 지정된 포트로 TCP 연결시도 실패시 컨테이너가 다시 시작  
C. Exec 프로브는 컨테이너 내부에 임의의 명령을 실행 종료후 상태코드 0 이 아니면 오류로 간주  

#### 1.2 HTTP 기반 프로브 생성

`kubia-liveness-probe.yml`  
```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```
`kubectl create -f kubia-liveness-probe.yml`

#### 1.3 동작중인 라이브니스 프로브 보기
`kubectl get po kubi-liveness`  

`kubectl get pods`

`kubectl k kubia-liveness --previous`

`kubectl describe po kubia-liveness`

`kubectl delete -f kubia-liveness-probe.yml`
- exit code 137 128 + 9 

#### 1.4 라이브니스 프로브의 추가 속성 구성
- kubectl describe 라이브니스 프로브의 추가정보 표시  
`Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3` 


- yaml에 initialDelaySeconds : 15  15초 지연됨 

- 어플리케이션 시작시간을 고려하여 항상 초기 지연을 설정하는 것을 잊지 말자

- 종료 코드 137 은 외부 신호에 의해 종료 됐음을 알려준다. (종료 코드 128 + 9(SIGKILL))
- 143 = 128 + 15 (SIGTERM)

#### 1.5 효과적인 라이브니스 프로브 생성 
- 운영 환경에서 실행 중인 팟의 경우 라이브니스 프로브를 정의해야 한다. 그래야 쿠버네티스가 어플리케이션 생사여부를 알수 있다. 
- 프로브 구성시 /health 같은 특정 엔드포인트를 설정하고 인증이 없도록 한다. (DB가 문제인 경우도 고려해야 함)
- 라이브니스 프로브는 가볍게 만든다.
- 자바일 경우 Exec 프로브 대신 HTTP GET 프로브를 사용해야 한다. ? P. 157
- 프로브에서 반복 루푸를 구현하지 말라

## 2. 레플리케이션 컨트롤러 
- 팟이 항상 실행되도록 유지하는 쿠버네티스 리소스 
- 어떤 이유로든 팟이 사라지면 레플리케이션컨트롤러는 누락된 팟을 감지하고 대체 팟을 만든다. 
- P 159 그림 4.1 노드 실패시 레플리케이션 컨트롤러는 사라진 팟 B를 대체할 새로운 팟 B2 를 생성하지만 A는 빠이빠이
- 레플리케이션컨트롤러는 단일 팟만 관리하지만 일반적으로 레플리케이션 팟은 복제본을 만들고 관리할 수 있다  --> 레플리 케이션컨트롤러 이름 유래

### 2.1 레플리케이션 컨트롤러의 동작
- 실행중인 팟 목록을 지속적으로 모니터링 
- 유형의 실제 팟 수가 원하는 수와 일치 하는지 확인
- 너무 많으면 제거 적으면 복제 생성

> **복제본이 더많이 생성될경우**  
- 수동으로 동일한 유형의 팟을 만든경우
- 기존 팟 유형을 변경한 경우
- 원하는 팟 수를 줄였을 경우 등  

- 리플리케이션 컨트롤러는 팟 유형을 가지고 동작하지 않지만 특정 라벨 셀렉터와 일치하는 팟 세트에서 동작한다.
- 컨트롤러의 연결 고리 소개 (P.160 그림 )
- 리플리케이션컨트롤러 세가지 요소 (P.161 그림)
  - 레플리케이션 컨트롤러 범위에 있는 팟을 결정하는 라벨 셀렉터
  - 원하는 팟 수를 지정하는 복제본수
  - 새로운 팟 복제본을 만들테 사용되는 팟 템플릿
  - 복제본수 , 라벨 셀레터 , 팟 템플리 모두 언제나 수정 가능 하지만 복제수의 변경은 기존 팟에 영향을 미친다. 
- 컨트롤러의 라벨 셀렉터 또는 팟 템플릿 변경에 따른 효과
  - 라벨 셀렉터 및 팟 변경은 미치지 않는다.
    - 라벨 셀렉터 변경시 기존 팟이 레플리케이션 컨트롤러의 범위를 벗어나 해당 팟을 신경쓰지 않는다.
    - 레플리케이션 컨트롤러는 팟을 만든후 컨테이너 이미지, 환경변수, 기타사항을 신경쓰지 않는다.  따라서 새로운 팟에만 영향을 준다.
- 레플리케이션컨트롤러를 사용해 얻는 이점

### 2.2 레플리케이션 컨트롤러의 생성
- 팟이나 다른 쿠버네티스 리소스와 같이 쿠버네티스 API 서버로 JSON 또는 YAML 지시자로 레플리케이션컨트롤러를 생성 할 수 있다.
`kubia-rc.yml` 
```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```

- 이 컨트롤러는 항상 라벨 셀렉터 app=kubia 와 일치하는 세개의 팟 인스턴스 유지
- 생성시 팟 템플릿으로 새 팟 생성 
- 템플릿 팟 라벨과 컨트롤러 라벨 셀렉터와 일치해야 함 그러지 않으면 컨트롤러는 새 팟 무한생성
- 새 팟을 스핀업 시킨다 해도 원하는 복제본 수에 근접한 실제 복제수가 생성되지 않기 때문이다. 이러한 경우를 방지하기 위해 API 서버는 컨트롤러 정의를 검사하고 잘못된경우 수락하지 않는다.
- 셀렉터를 지정하지 않으면 팟 템플릿 라벨에서 자동으로 구성 (YAML 짧고 단순해짐)

`kubectl create -f kubia-rc.yml`

### 2.3 실행 중인 레플리케이션컨트롤러 보기
`kubectl get pods`

`kubectl delete pod {name}`

`kubectl get pods`

`kubectl get rc` : rc는 레플리케이션컨트롤러의 줄임말

`kubectl describe rc kubia` p .165

- (P.166 4.4) 팟이 사라지면 레플리케이션 컨트롤러는 팟 수가 적음을 알고 새 팟 생성

### 2.4 레플리케이션컨트롤러의 범위 안팎으로 팟 이동
### 2.5 팟 템플릿 변경
### 2.6 수평 팟 스케일링
### 2.7 레플리케이션 컨트롤러 삭제
